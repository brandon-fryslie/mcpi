# MCPI v0.6.0 Template Discovery Engine - Implementation Plan

**Generated**: 2025-11-18 06:03:44
**Source STATUS**: STATUS-2025-11-18-060044.md (Zero-Optimism Protocol)
**Spec Reference**: FEATURE_PROPOSAL_POST_v0.5.0_2025-11-17.md (Template Discovery Section)
**Target Release**: v0.6.0
**Timeline**: 2-3 weeks (15 working days, ~84 hours)

---

## Document Provenance

- **Source STATUS File**: STATUS-2025-11-18-060044.md (2025-11-18 06:00:44)
- **Specification**: FEATURE_PROPOSAL_POST_v0.5.0_2025-11-17.md
- **Spec Last Modified**: 2025-11-17
- **Planning File Generated**: 2025-11-18 06:03:44
- **Current State**: v0.5.0 = 100% SHIPPED | v0.6.0 = 5% STUB ONLY (273 lines scaffolding, 0 functional)

---

## Executive Summary

### Current Reality (Brutal Facts)

**v0.5.0 Configuration Templates**: 100% COMPLETE âœ…
- 12 production templates working
- 95%+ test coverage, 94/94 tests passing
- User feedback: "This is amazing!"
- Zero known issues

**v0.6.0 Template Discovery Engine**: 5% SCAFFOLDING, 0% FUNCTIONAL ğŸ”´
- **Stub files created**: discovery.py (101 lines), recommender.py (172 lines)
- **All methods**: Raise NotImplementedError with helpful messages
- **Test status**: 30 TDD tests written, 0 passing (expected - waiting for implementation)
- **Blocker count**: ZERO - ready to implement
- **Risk level**: LOW

**Reality Check**: This is Day 0.5. Scaffolding exists but contains zero working functionality. All real work lies ahead. Estimated 2-3 weeks of focused implementation required.

### Ship Criteria for v0.6.0

Must achieve ALL before shipping:
- [ ] All 21 implementation tasks completed (0/21 done)
- [ ] All 30 discovery tests passing (0/30 passing)
- [ ] Test coverage >= 95% on new code (0% current)
- [ ] No regressions (maintain 96.8% overall pass rate)
- [ ] All 12 templates have metadata (0/12 done)
- [ ] Manual E2E verification passes (0/8 scenarios verified)
- [ ] Documentation updated (0/4 files updated)
- [ ] Performance < 500ms end-to-end (not measured)

---

## 1. Implementation Backlog by Priority

### P0 (Critical - Week 1: Detection Infrastructure)

#### P0-1: Implement Docker Detection

**Status**: NOT STARTED
**Effort**: 4 hours
**Dependencies**: None
**Spec Reference**: Feature Proposal Section 3.1 (Project Detection)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 1

**Description**:
Implement `_detect_docker()` method in `ProjectDetector` to parse docker-compose.yml and detect Docker usage. This is the foundational detection capability that will prove the YAML parsing approach works.

**Acceptance Criteria**:
- [ ] `_detect_docker()` method implemented and working
- [ ] Parses docker-compose.yml using PyYAML
- [ ] Extracts service names from services section
- [ ] Sets has_docker_compose flag when file exists
- [ ] Sets has_docker flag when Dockerfile exists
- [ ] Gracefully handles corrupted/invalid YAML files
- [ ] Test passes: test_detect_docker_compose_project
- [ ] Manual verification works with real docker-compose.yml

**Technical Notes**:
```python
# Implementation approach in src/mcpi/templates/discovery.py
def _detect_docker(self, context: ProjectContext, path: Path) -> None:
    import yaml

    # Check docker-compose.yml
    compose_file = path / "docker-compose.yml"
    if compose_file.exists():
        context.has_docker_compose = True
        try:
            with open(compose_file) as f:
                data = yaml.safe_load(f)
            services = data.get("services", {})
            context.docker_services = list(services.keys())
        except Exception:
            # Graceful failure for corrupted YAML
            context.docker_services = []

    # Check Dockerfile
    if (path / "Dockerfile").exists():
        context.has_docker = True
```

**Verification**:
```bash
# Create test project
mkdir -p /tmp/test_docker_project
cat > /tmp/test_docker_project/docker-compose.yml <<EOF
services:
  postgres:
    image: postgres:15
  redis:
    image: redis:7
EOF

# Test detection
python -c "
from pathlib import Path
from mcpi.templates.discovery import ProjectDetector
d = ProjectDetector()
ctx = d.detect(Path('/tmp/test_docker_project'))
print(f'Docker Compose: {ctx.has_docker_compose}')
print(f'Services: {ctx.docker_services}')
"
# Expected: Docker Compose: True, Services: ['postgres', 'redis']
```

---

#### P0-2: Implement Language Detection

**Status**: NOT STARTED
**Effort**: 3 hours
**Dependencies**: None
**Spec Reference**: Feature Proposal Section 3.1 (Project Detection)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 1

**Description**:
Implement `_detect_language()` method to identify project language from marker files (package.json, requirements.txt, go.mod, Cargo.toml, etc.).

**Acceptance Criteria**:
- [ ] `_detect_language()` method implemented
- [ ] Detects Node.js (package.json)
- [ ] Detects Python (requirements.txt, pyproject.toml, setup.py)
- [ ] Detects Go (go.mod)
- [ ] Detects Rust (Cargo.toml)
- [ ] Handles multiple languages in same project
- [ ] Tests pass: test_detect_nodejs_project, test_detect_python_project
- [ ] Manual verification with real projects

**Technical Notes**:
```python
def _detect_language(self, context: ProjectContext, path: Path) -> None:
    languages = []

    if (path / "package.json").exists():
        languages.append("javascript")
    if any((path / f).exists() for f in ["requirements.txt", "pyproject.toml", "setup.py"]):
        languages.append("python")
    if (path / "go.mod").exists():
        languages.append("go")
    if (path / "Cargo.toml").exists():
        languages.append("rust")

    context.languages = languages
```

---

#### P0-3: Implement Database Detection

**Status**: NOT STARTED
**Effort**: 2 hours
**Dependencies**: P0-1 (uses docker_services)
**Spec Reference**: Feature Proposal Section 3.1 (Project Detection)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 1

**Description**:
Implement `_detect_database()` method to identify databases from Docker Compose services (postgres, mysql, mongodb, redis).

**Acceptance Criteria**:
- [ ] `_detect_database()` method implemented
- [ ] Detects postgres from service name or image
- [ ] Detects mysql from service name or image
- [ ] Detects mongodb from service name or image
- [ ] Detects redis from service name or image
- [ ] Handles multiple databases
- [ ] Test passes: test_detect_database_from_docker_services
- [ ] Manual verification works

**Technical Notes**:
```python
def _detect_database(self, context: ProjectContext, path: Path) -> None:
    databases = set()

    # Detect from docker services
    for service in context.docker_services:
        service_lower = service.lower()
        if "postgres" in service_lower:
            databases.add("postgres")
        elif "mysql" in service_lower or "mariadb" in service_lower:
            databases.add("mysql")
        elif "mongo" in service_lower:
            databases.add("mongodb")
        elif "redis" in service_lower:
            databases.add("redis")

    context.databases = list(databases)
```

---

#### P0-4: Implement ProjectDetector.detect() Orchestrator

**Status**: NOT STARTED
**Effort**: 2 hours
**Dependencies**: P0-1, P0-2, P0-3
**Spec Reference**: Feature Proposal Section 3.1 (Project Detection)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 1

**Description**:
Implement main `detect()` method that orchestrates all detection methods and returns complete ProjectContext.

**Acceptance Criteria**:
- [ ] `detect()` method implemented
- [ ] Calls all detection methods in order
- [ ] Returns complete ProjectContext
- [ ] Handles path validation (must exist)
- [ ] No crashes on empty directories
- [ ] All 7 detection tests passing
- [ ] Manual E2E verification works

**Technical Notes**:
```python
def detect(self, project_path: Path) -> ProjectContext:
    if not project_path.exists():
        raise ValueError(f"Project path does not exist: {project_path}")

    context = ProjectContext(project_path=project_path)

    self._detect_docker(context, project_path)
    self._detect_language(context, project_path)
    self._detect_database(context, project_path)

    return context
```

---

#### P0-5: Extend ServerTemplate Model

**Status**: NOT STARTED
**Effort**: 1 hour
**Dependencies**: None
**Spec Reference**: Feature Proposal Section 3.2 (Template Metadata)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 3

**Description**:
Add three new optional fields to ServerTemplate Pydantic model: best_for, keywords, recommendations.

**Acceptance Criteria**:
- [ ] Fields added to ServerTemplate model in models.py
- [ ] All fields are optional (backward compatible)
- [ ] Pydantic validation works
- [ ] Existing templates still load (no breaking changes)
- [ ] Type hints correct
- [ ] Tests pass

**Technical Notes**:
```python
# In src/mcpi/templates/models.py
class ServerTemplate(BaseModel):
    name: str
    description: str
    server_id: str
    scope: str
    priority: Literal["high", "medium", "low"]
    config: dict[str, Any]
    prompts: list[PromptDefinition] = Field(default_factory=list)
    notes: str = Field(default="")

    # v0.6.0 Discovery metadata (all optional for backward compatibility)
    best_for: list[str] = Field(default_factory=list, description="Use cases this template is optimized for")
    keywords: list[str] = Field(default_factory=list, description="Keywords for matching against project characteristics")
    recommendations: dict[str, Any] = Field(default_factory=dict, description="Recommendation scoring hints")
```

---

#### P0-6: Update Template Metadata (12 Templates)

**Status**: NOT STARTED
**Effort**: 6 hours (30 min per template)
**Dependencies**: P0-5
**Spec Reference**: Feature Proposal Section 3.2 (Template Metadata)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 3

**Description**:
Update all 12 existing templates with discovery metadata (best_for, keywords, recommendations).

**Acceptance Criteria**:
- [ ] All 12 templates updated with metadata
- [ ] postgres/docker: keywords include docker, container, docker-compose
- [ ] postgres/local-development: keywords include local, development, non-docker
- [ ] github templates: keywords include version-control, github, git
- [ ] filesystem templates: keywords include local-files, project-files
- [ ] All templates still load correctly
- [ ] Validation passes
- [ ] Manual review confirms quality

**Technical Notes**:
Example for postgres/docker.yaml:
```yaml
name: docker
description: "PostgreSQL running in Docker container"
server_id: postgres
scope: user-global
priority: high

best_for:
  - "Projects using Docker Compose"
  - "Containerized development environments"
  - "Teams sharing consistent database setup"

keywords:
  - docker
  - docker-compose
  - container
  - postgres
  - database

recommendations:
  required_characteristics:
    - has_docker_compose: true
  score_boost: 50  # High boost when docker-compose detected

config:
  command: npx
  args: ["-y", "@modelcontextprotocol/server-postgres"]
  env:
    POSTGRES_CONNECTION_STRING: ""
# ... rest of template
```

---

### P0 (Critical - Week 2: Recommendation Engine)

#### P0-7: Implement Scoring Algorithm

**Status**: NOT STARTED
**Effort**: 6 hours
**Dependencies**: P0-5, P0-6
**Spec Reference**: Feature Proposal Section 3.3 (Scoring Algorithm)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 2

**Description**:
Implement `_score_template()` method with confidence-based scoring algorithm that matches project context against template metadata.

**Acceptance Criteria**:
- [ ] `_score_template()` method implemented
- [ ] Scores based on keyword matching
- [ ] Applies score_boost from recommendations
- [ ] Returns confidence score 0-100
- [ ] Includes reasoning in result
- [ ] Tests pass: test_score_docker_template_with_docker_project
- [ ] Manual tuning shows reasonable scores

**Technical Notes**:
```python
def _score_template(
    self, template: ServerTemplate, context: ProjectContext
) -> tuple[int, list[str]]:
    """Score template against project context.

    Returns:
        (confidence_score, reasoning_points)
    """
    score = 0
    reasons = []

    # Keyword matching (5 points per match)
    matched_keywords = []
    for keyword in template.keywords:
        if keyword == "docker" and context.has_docker_compose:
            matched_keywords.append(keyword)
        elif keyword in context.languages:
            matched_keywords.append(keyword)
        elif keyword in context.databases:
            matched_keywords.append(keyword)

    if matched_keywords:
        score += len(matched_keywords) * 5
        reasons.append(f"Matched keywords: {', '.join(matched_keywords)}")

    # Apply template score_boost
    if "score_boost" in template.recommendations:
        boost = template.recommendations["score_boost"]
        score += boost
        reasons.append(f"Template optimized for this use case (+{boost})")

    # Cap at 100
    score = min(score, 100)

    return score, reasons
```

---

#### P0-8: Implement TemplateRecommender.recommend()

**Status**: NOT STARTED
**Effort**: 4 hours
**Dependencies**: P0-4, P0-7
**Spec Reference**: Feature Proposal Section 3.3 (Recommendation API)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 2

**Description**:
Implement main `recommend()` method that uses ProjectDetector and scoring algorithm to return ranked list of template recommendations.

**Acceptance Criteria**:
- [ ] `recommend()` method implemented
- [ ] Uses ProjectDetector to analyze project
- [ ] Scores all templates for server_id
- [ ] Returns ranked list of TemplateRecommendation
- [ ] Includes confidence and reasoning
- [ ] Filters out low-confidence recommendations (< 10)
- [ ] Tests pass: test_recommend_returns_ranked_list
- [ ] Manual verification shows sensible recommendations

**Technical Notes**:
```python
def recommend(
    self,
    server_id: str,
    project_path: Path | None = None,
    min_confidence: int = 10,
) -> list[TemplateRecommendation]:
    """Recommend templates for server based on project context."""
    # Detect project characteristics
    if project_path:
        detector = ProjectDetector()
        context = detector.detect(project_path)
    else:
        context = ProjectContext(project_path=Path.cwd())

    # Get all templates for server
    templates = self.template_manager.list_templates(server_id)

    # Score each template
    recommendations = []
    for template in templates:
        confidence, reasons = self._score_template(template, context)

        if confidence >= min_confidence:
            recommendations.append(
                TemplateRecommendation(
                    template_name=template.name,
                    server_id=server_id,
                    confidence=confidence,
                    reasons=reasons,
                    template=template,
                )
            )

    # Sort by confidence (descending)
    recommendations.sort(key=lambda r: r.confidence, reverse=True)

    return recommendations
```

---

#### P0-9: Create Factory Functions

**Status**: NOT STARTED
**Effort**: 2 hours
**Dependencies**: P0-8
**Spec Reference**: CLAUDE.md DIP Implementation Section
**Status Reference**: STATUS-2025-11-18-060044.md Section 5.2

**Description**:
Create DIP-compliant factory functions for creating TemplateRecommender instances (production and test).

**Acceptance Criteria**:
- [ ] `create_default_template_recommender()` implemented
- [ ] `create_test_template_recommender()` implemented
- [ ] Factories follow established pattern
- [ ] Production factory uses default template manager
- [ ] Test factory accepts injected template manager
- [ ] Tests use test factory
- [ ] Documentation updated

**Technical Notes**:
```python
# In src/mcpi/templates/recommender.py

def create_default_template_recommender() -> TemplateRecommender:
    """Create recommender with default dependencies (production use)."""
    from mcpi.templates.template_manager import create_default_template_manager
    template_manager = create_default_template_manager()
    return TemplateRecommender(template_manager=template_manager)

def create_test_template_recommender(
    template_manager: TemplateManager,
) -> TemplateRecommender:
    """Create recommender with injected dependencies (test use)."""
    return TemplateRecommender(template_manager=template_manager)
```

---

### P1 (High - Week 3: CLI Integration)

#### P1-1: Add --recommend Flag to CLI

**Status**: NOT STARTED
**Effort**: 3 hours
**Dependencies**: P0-8, P0-9
**Spec Reference**: Feature Proposal Section 3.4 (CLI Integration)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 4

**Description**:
Add `--recommend` flag to `mcpi add` command that triggers template recommendation flow.

**Acceptance Criteria**:
- [ ] `--recommend` flag added to add command
- [ ] Flag is optional (defaults to False)
- [ ] Mutually exclusive checks with --template
- [ ] Help text clear and helpful
- [ ] CLI tests updated
- [ ] Manual testing confirms flag works

**Technical Notes**:
```python
# In src/mcpi/cli.py
@click.option(
    "--recommend",
    is_flag=True,
    help="Recommend templates based on project analysis",
)
def add(
    server: str,
    scope: str,
    template: str | None,
    recommend: bool,
    list_templates: bool,
    ...
):
    if recommend and template:
        console.print("[red]Cannot use --recommend with --template[/red]")
        raise click.Abort()

    if recommend:
        # Trigger recommendation flow
        _handle_recommendation_flow(server, scope, ...)
    elif template:
        # Use specified template
        _handle_template_flow(server, scope, template, ...)
    else:
        # No template/recommendation
        _handle_basic_add(server, scope, ...)
```

---

#### P1-2: Implement Rich Recommendation Display

**Status**: NOT STARTED
**Effort**: 4 hours
**Dependencies**: P1-1
**Spec Reference**: Feature Proposal Section 3.4 (CLI Integration)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 4

**Description**:
Implement Rich console output to display template recommendations in formatted table with confidence, reasons, and interactive selection.

**Acceptance Criteria**:
- [ ] Recommendation table with columns: Rank, Template, Confidence, Reasons
- [ ] Color-coded confidence (green=high, yellow=medium, red=low)
- [ ] Interactive user prompt to select recommendation
- [ ] Clear "show all templates" option
- [ ] Beautiful, professional output
- [ ] Manual testing confirms good UX

**Technical Notes**:
```python
from rich.table import Table
from rich.prompt import Prompt

def _display_recommendations(recommendations: list[TemplateRecommendation]) -> str | None:
    """Display recommendations and get user selection."""
    console.print("\n[bold]Recommended Templates:[/bold]\n")

    table = Table(show_header=True, header_style="bold cyan")
    table.add_column("Rank", width=6)
    table.add_column("Template", width=25)
    table.add_column("Confidence", width=12)
    table.add_column("Why", width=50)

    for idx, rec in enumerate(recommendations, 1):
        confidence_color = (
            "green" if rec.confidence >= 70
            else "yellow" if rec.confidence >= 40
            else "red"
        )
        table.add_row(
            str(idx),
            rec.template_name,
            f"[{confidence_color}]{rec.confidence}%[/{confidence_color}]",
            ", ".join(rec.reasons[:2]),  # Show first 2 reasons
        )

    console.print(table)

    # Interactive selection
    choice = Prompt.ask(
        "\nSelect template number (or 'all' to see all templates)",
        default="1",
    )

    if choice.lower() == "all":
        return None

    try:
        idx = int(choice) - 1
        return recommendations[idx].template_name
    except (ValueError, IndexError):
        console.print("[red]Invalid selection[/red]")
        return None
```

---

#### P1-3: Wire Up Recommendation Flow

**Status**: NOT STARTED
**Effort**: 3 hours
**Dependencies**: P1-1, P1-2
**Spec Reference**: Feature Proposal Section 3.4 (CLI Integration)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 4

**Description**:
Integrate recommendation engine with CLI, connecting --recommend flag to full recommendation workflow.

**Acceptance Criteria**:
- [ ] `_handle_recommendation_flow()` implemented
- [ ] Creates recommender using factory
- [ ] Gets recommendations for server_id
- [ ] Displays recommendations using Rich
- [ ] Handles user selection
- [ ] Falls back to normal add if declined
- [ ] Error handling for no recommendations
- [ ] All CLI integration tests passing

**Technical Notes**:
```python
def _handle_recommendation_flow(server: str, scope: str, ...):
    """Handle --recommend flag workflow."""
    from mcpi.templates.recommender import create_default_template_recommender

    try:
        recommender = create_default_template_recommender()
        recommendations = recommender.recommend(
            server_id=server,
            project_path=Path.cwd(),
        )

        if not recommendations:
            console.print(f"[yellow]No recommendations for {server}[/yellow]")
            console.print("Showing all available templates...\n")
            # Fall back to list templates
            return _handle_list_templates(server)

        selected = _display_recommendations(recommendations)

        if selected:
            # Proceed with selected template
            return _handle_template_flow(server, scope, selected, ...)
        else:
            # User wants to see all
            return _handle_list_templates(server)

    except Exception as e:
        console.print(f"[red]Recommendation failed: {e}[/red]")
        console.print("Falling back to template list...\n")
        return _handle_list_templates(server)
```

---

### P2 (Medium - Testing & Documentation)

#### P2-1: Unit Tests for Detection

**Status**: 30% (tests written, 0 passing)
**Effort**: 4 hours
**Dependencies**: P0-1, P0-2, P0-3, P0-4
**Spec Reference**: Feature Proposal Section 4 (Testing Strategy)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 5

**Description**:
Ensure all detection unit tests pass by implementing detection methods correctly.

**Acceptance Criteria**:
- [ ] test_detect_docker_compose_project PASSING
- [ ] test_detect_docker_services PASSING
- [ ] test_detect_nodejs_project PASSING
- [ ] test_detect_python_project PASSING
- [ ] test_detect_multiple_languages PASSING
- [ ] test_detect_database_from_docker_services PASSING
- [ ] test_detector_graceful_failure PASSING
- [ ] 7/7 detection tests passing (100%)

---

#### P2-2: Unit Tests for Scoring

**Status**: 30% (tests written, 0 passing)
**Effort**: 3 hours
**Dependencies**: P0-7
**Spec Reference**: Feature Proposal Section 4 (Testing Strategy)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 5

**Description**:
Ensure all scoring unit tests pass by implementing scoring algorithm correctly.

**Acceptance Criteria**:
- [ ] test_score_docker_template_with_docker_project PASSING
- [ ] test_score_local_template_without_docker PASSING
- [ ] test_scoring_threshold PASSING
- [ ] test_ranking_by_confidence PASSING
- [ ] test_score_keyword_matching PASSING
- [ ] 5/5 scoring tests passing (100%)

---

#### P2-3: Integration Tests E2E

**Status**: 30% (tests written, 0 passing)
**Effort**: 4 hours
**Dependencies**: P0-8, P1-3
**Spec Reference**: Feature Proposal Section 4 (Testing Strategy)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 5

**Description**:
Ensure all E2E integration tests pass, verifying full workflow from detection to recommendation to CLI.

**Acceptance Criteria**:
- [ ] test_recommend_docker_postgres_for_docker_compose_project PASSING
- [ ] test_recommend_local_postgres_for_non_docker_project PASSING
- [ ] test_cli_recommend_flag PASSING
- [ ] test_recommendation_display PASSING
- [ ] test_no_recommendations_fallback PASSING
- [ ] All CLI integration tests passing (15/15)
- [ ] All E2E tests passing (2/2)

---

#### P2-4: Edge Case Testing

**Status**: 30% (tests written, 0 passing)
**Effort**: 2 hours
**Dependencies**: All implementation tasks
**Spec Reference**: Feature Proposal Section 4 (Testing Strategy)
**Status Reference**: STATUS-2025-11-18-060044.md Section 1.2 Phase 5

**Description**:
Verify edge cases handled gracefully (empty project, corrupted files, no recommendations, etc.).

**Acceptance Criteria**:
- [ ] Empty project directory handled
- [ ] Corrupted docker-compose.yml handled
- [ ] No templates available handled
- [ ] Invalid project path handled
- [ ] All edge case tests passing

---

#### P2-5: Documentation Updates

**Status**: NOT STARTED
**Effort**: 4 hours
**Dependencies**: All implementation tasks
**Spec Reference**: Feature Proposal Section 5 (Documentation)
**Status Reference**: STATUS-2025-11-18-060044.md Section 8.4

**Description**:
Update project documentation to reflect new --recommend feature, including examples, usage guide, and architecture notes.

**Acceptance Criteria**:
- [ ] README.md updated with --recommend examples
- [ ] CLAUDE.md updated with Template Discovery section
- [ ] Template metadata authoring guide created
- [ ] CHANGELOG entry for v0.6.0
- [ ] CLI help text updated
- [ ] All examples verified working

---

#### P2-6: Performance Testing

**Status**: NOT STARTED
**Effort**: 2 hours
**Dependencies**: P0-8, P1-3
**Spec Reference**: Feature Proposal Section 6 (Performance)
**Status Reference**: STATUS-2025-11-18-060044.md Section 3.2

**Description**:
Verify recommendation flow completes in < 500ms end-to-end, meeting performance targets.

**Acceptance Criteria**:
- [ ] Detection phase < 100ms
- [ ] Scoring phase < 50ms
- [ ] Total E2E < 500ms
- [ ] Performance tests added
- [ ] Benchmarks documented

---

### P3 (Low - Final Polish)

#### P3-1: Final Polish & Ship Checklist

**Status**: NOT STARTED
**Effort**: 4 hours
**Dependencies**: All P0, P1, P2 tasks
**Spec Reference**: Feature Proposal Ship Criteria
**Status Reference**: STATUS-2025-11-18-060044.md Section 10.3

**Description**:
Final verification, manual E2E testing, and ship checklist completion before v0.6.0 release.

**Acceptance Criteria**:
- [ ] All 21 tasks completed
- [ ] All 30 tests passing (100%)
- [ ] Test coverage >= 95%
- [ ] No regressions (96.8%+ overall pass rate maintained)
- [ ] All 12 templates have metadata
- [ ] Manual E2E verification (8 scenarios)
- [ ] Documentation complete
- [ ] Performance targets met
- [ ] Git history clean
- [ ] CHANGELOG updated
- [ ] Version bumped to 0.6.0
- [ ] Ready to tag and release

---

## 2. Dependency Graph

```
WEEK 1: Detection Infrastructure
P0-1 (Docker Detection) â”€â”€â”
P0-2 (Language Detection) â”¼â”€â†’ P0-4 (detect() orchestrator) â”€â”€â”
P0-3 (Database Detection) â”˜                                   â”‚
                                                               â”‚
P0-5 (Extend ServerTemplate) â”€â”€â†’ P0-6 (Update 12 templates) â”€â”€â”¤
                                                               â”‚
                                                               â”œâ”€â†’ WEEK 2
                                                               â”‚
WEEK 2: Recommendation Engine                                 â”‚
P0-6 (Template metadata) â”€â”€â†’ P0-7 (Scoring algorithm) â”€â”€â”€â”€â”€â”€â”€â”¤
P0-4 (ProjectDetector) â”€â”€â”€â”€â”€â”€â”˜                                â”‚
                                                               â”‚
P0-7 (Scoring) â”€â”€â†’ P0-8 (recommend() method) â”€â”€â†’ P0-9 (Factories) â”€â”€â”
                                                                      â”‚
                                                                      â”œâ”€â†’ WEEK 3
                                                                      â”‚
WEEK 3: CLI Integration                                              â”‚
P0-9 (Factories) â”€â”€â†’ P1-1 (--recommend flag) â”€â”€â†’ P1-2 (Rich display) â”€â”€â†’ P1-3 (Wire up flow)
                                                                                     â”‚
Testing & Documentation (parallel with Week 3)                                      â”‚
P2-1 (Detection tests) â”€â”€â”                                                          â”‚
P2-2 (Scoring tests)   â”€â”€â”¼â”€â†’ P2-3 (E2E tests) â”€â”€â†’ P2-4 (Edge cases) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
P2-5 (Documentation)   â”€â”€â”˜                                                          â”‚
P2-6 (Performance)                                                                  â”‚
                                                                                    â”‚
                                                                                    â”œâ”€â†’ SHIP
                                                                                    â”‚
P3-1 (Final polish & checklist) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Sprint Plan (3 Weeks)

### Week 1: Detection Infrastructure (Days 1-5)

**Goal**: Complete project detection, extend models, update templates

**Tasks**: P0-1, P0-2, P0-3, P0-4, P0-5, P0-6, P2-1 (partial)

**Daily Breakdown**:
- **Day 1**: P0-1 Docker detection (4h) + manual verification (1h)
- **Day 2**: P0-2 Language detection (3h) + P0-3 Database detection (2h)
- **Day 3**: P0-4 detect() orchestrator (2h) + P0-5 Extend model (1h) + start P0-6 (2h)
- **Day 4**: P0-6 Continue template updates (5h)
- **Day 5**: P0-6 Finish templates (1h) + P2-1 Detection tests passing (4h)

**Exit Criteria**:
- [ ] All detection methods working
- [ ] 7/7 detection tests passing
- [ ] ServerTemplate model extended
- [ ] All 12 templates updated with metadata
- [ ] Manual verification with 3 real projects

**Checkpoint**: End of Week 1 - If detection not working, reassess timeline

---

### Week 2: Recommendation Engine (Days 6-10)

**Goal**: Complete scoring algorithm, recommendation API, factories, tests

**Tasks**: P0-7, P0-8, P0-9, P2-2, P2-3 (partial)

**Daily Breakdown**:
- **Day 6**: P0-7 Scoring algorithm implementation (6h)
- **Day 7**: P0-7 Scoring algorithm tuning (2h) + P2-2 Scoring tests (3h)
- **Day 8**: P0-8 recommend() method (4h) + manual verification (1h)
- **Day 9**: P0-9 Factory functions (2h) + P2-3 E2E tests partial (3h)
- **Day 10**: P2-3 E2E tests complete (5h)

**Exit Criteria**:
- [ ] Scoring algorithm working
- [ ] recommend() method returns sensible results
- [ ] Factory functions created
- [ ] 5/5 scoring tests passing
- [ ] 12/12 recommendation tests passing
- [ ] Manual verification shows good recommendations

**Checkpoint**: End of Week 2 - Verify recommendation quality is acceptable

---

### Week 3: CLI Integration & Ship (Days 11-15)

**Goal**: CLI integration, documentation, final polish, ship v0.6.0

**Tasks**: P1-1, P1-2, P1-3, P2-4, P2-5, P2-6, P3-1

**Daily Breakdown**:
- **Day 11**: P1-1 --recommend flag (3h) + P1-2 Rich display (2h)
- **Day 12**: P1-2 Rich display polish (2h) + P1-3 Wire up flow (3h)
- **Day 13**: P2-4 Edge cases (2h) + P2-5 Documentation (3h)
- **Day 14**: P2-5 Documentation finish (1h) + P2-6 Performance (2h) + P3-1 Start checklist (2h)
- **Day 15**: P3-1 Final polish (2h) + Manual E2E (2h) + Ship v0.6.0 (1h)

**Exit Criteria**:
- [ ] All CLI integration tests passing (15/15)
- [ ] All 30 discovery tests passing (100%)
- [ ] Documentation complete (4 files)
- [ ] Performance < 500ms
- [ ] Manual E2E verification passes (8/8 scenarios)
- [ ] Ship checklist complete

**Ship**: End of Day 15 - Tag v0.6.0 and release

---

## 4. Risk Mitigation

### Implementation Risks

| Risk | Probability | Impact | Mitigation | Owner |
|------|------------|--------|------------|-------|
| Docker Compose parsing fails | MEDIUM | MEDIUM | Use PyYAML with exception handling, test with real files | P0-1 |
| False positive detection | MEDIUM | LOW | Conservative scoring, show all templates option | P0-7 |
| Scoring algorithm poor quality | MEDIUM | MEDIUM | Extensive testing, tunable weights, user feedback | P0-7 |
| Performance issues | LOW | LOW | Simple file checks, no network calls, benchmark | P2-6 |
| Template metadata inconsistent | LOW | MEDIUM | Validation in Pydantic model, manual review | P0-6 |

### Schedule Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Detection takes longer than 1 week | LOW | MEDIUM | Start with Docker (highest value), defer language/DB if needed |
| Scoring algorithm needs tuning | MEDIUM | LOW | Allocate buffer time in Week 2, accept "good enough" |
| CLI integration complex | LOW | MEDIUM | Keep UI simple, defer polish if needed |
| Testing finds bugs | MEDIUM | MEDIUM | Fix incrementally, prioritize P0 bugs |

---

## 5. Success Metrics

### Ship Criteria (Must Achieve ALL)

- [ ] All 21 implementation tasks completed
- [ ] All 30 discovery tests passing (100%)
- [ ] Test coverage >= 95% on new code
- [ ] No regressions (maintain 96.8%+ overall pass rate)
- [ ] All 12 templates have metadata
- [ ] Manual E2E verification passes (8/8 scenarios)
- [ ] Documentation updated (4/4 files)
- [ ] Performance < 500ms end-to-end

### Target Metrics (Projected)

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| Adoption Rate | 40%+ | % users using --recommend flag |
| Acceptance Rate | 85%+ | % accepting top recommendation |
| Time Savings | 75% | Template selection time (45s â†’ 10s) |
| Test Coverage | 95%+ | Coverage on new code |
| User Satisfaction | High | Feedback, GitHub stars |

---

## 6. Manual Verification Checklist

### E2E Scenarios (Must Verify Before Ship)

1. [ ] **Docker Compose + postgres service** â†’ postgres/docker template (high confidence)
   - Setup: Create project with docker-compose.yml containing postgres service
   - Run: `mcpi add postgres --recommend`
   - Expect: postgres/docker as #1 recommendation with 70%+ confidence

2. [ ] **Node.js project without Docker** â†’ postgres/local-development template
   - Setup: Create project with package.json, no Docker
   - Run: `mcpi add postgres --recommend`
   - Expect: postgres/local-development as #1 recommendation

3. [ ] **Empty project** â†’ show all templates (no recommendations)
   - Setup: Empty directory
   - Run: `mcpi add postgres --recommend`
   - Expect: Message "No recommendations" + fall back to template list

4. [ ] **Invalid server ID** â†’ clear error message
   - Run: `mcpi add invalid-server --recommend`
   - Expect: Error message about server not found

5. [ ] **Corrupted docker-compose.yml** â†’ graceful fallback
   - Setup: Create invalid YAML file
   - Run: `mcpi add postgres --recommend`
   - Expect: No crash, falls back to template list

6. [ ] **User accepts recommendation** â†’ proceeds with that template
   - Run: `mcpi add postgres --recommend`
   - Select: Choose #1 recommendation
   - Expect: Proceeds to install that template

7. [ ] **User declines recommendation** â†’ shows full template list
   - Run: `mcpi add postgres --recommend`
   - Select: Choose "all"
   - Expect: Shows full template list

8. [ ] **--template flag overrides --recommend** â†’ error message
   - Run: `mcpi add postgres --recommend --template docker`
   - Expect: Error about mutually exclusive flags

---

## 7. Rollback Plan

### If Week 1 Fails (Detection Not Working)

**Decision Point**: End of Week 1
**Criteria**: Detection tests not passing, detection logic broken

**Options**:
1. Extend timeline by 1 week (4 weeks total)
2. Simplify detection (Docker only, skip language/database)
3. Defer v0.6.0, ship hotfix for critical bugs instead

**Recommendation**: Simplify detection first, then extend if needed

### If Week 2 Fails (Scoring Poor Quality)

**Decision Point**: End of Week 2
**Criteria**: Recommendations don't make sense, acceptance rate < 50% in manual testing

**Options**:
1. Simplify scoring (keyword matching only)
2. Add manual tuning phase (Week 4)
3. Ship as experimental feature (flag behind --experimental)

**Recommendation**: Simplify scoring, accept "good enough" for v0.6.0

### If Week 3 Fails (CLI Integration Complex)

**Decision Point**: Day 13
**Criteria**: CLI integration not working, tests failing

**Options**:
1. Simplify UI (basic table, no interactive selection)
2. Ship CLI integration in v0.6.1 (recommendation API only in v0.6.0)
3. Extend timeline by 3 days

**Recommendation**: Simplify UI, ship basic version

---

## 8. Post-Ship Review

### After v0.6.0 Ships

**Collect Metrics** (30 days after ship):
- Adoption rate (% using --recommend)
- Acceptance rate (% accepting top recommendation)
- User feedback (GitHub issues, comments)
- Performance data (actual latency)
- Error rate (crashes, exceptions)

**Evaluate Success**:
- Did we hit target metrics?
- What worked well?
- What needs improvement?
- Should we invest in v0.6.1 enhancements?

**Next Steps**:
- Plan v0.6.1 (framework detection, cloud detection)
- Plan v0.7.0 (Template Test Drive)
- Plan v0.8.0 (Template Workflows)

---

## 9. References

### Planning Documents
- **Source STATUS**: STATUS-2025-11-18-060044.md
- **Feature Proposal**: FEATURE_PROPOSAL_POST_v0.5.0_2025-11-17.md
- **Previous Plan**: PLAN-TEMPLATE-DISCOVERY-2025-11-17-132624.md
- **Previous Sprint**: SPRINT-TEMPLATE-DISCOVERY-2025-11-17-132624.md
- **Test Report**: TEMPLATE-DISCOVERY-TEST-REPORT-2025-11-17.md

### Specification
- **Architecture**: CLAUDE.md (Project Architecture section)
- **DIP Implementation**: CLAUDE.md (Dependency Inversion Principle section)
- **Template System**: CLAUDE.md (Configuration Templates System section)

### Test Files
- `tests/test_template_discovery_functional.py` (14 tests, 0 passing)
- `tests/test_template_recommendation_cli.py` (19 tests, 0 passing)

---

**Last Updated**: 2025-11-18 06:03:44
**Status**: v0.5.0 Shipped âœ… | v0.6.0 Day 0.5 - Ready to Implement ğŸš€
**Next Action**: Start P0-1 (Docker Detection) - First test passing by EOD
**First Checkpoint**: End of Day 2 - Verify Docker detection works
**Ship Target**: 2025-12-08 (3 weeks from start)
