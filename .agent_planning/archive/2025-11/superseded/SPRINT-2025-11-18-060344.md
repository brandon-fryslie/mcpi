# MCPI v0.6.0 Template Discovery Engine - Sprint Plan

**Generated**: 2025-11-18 06:03:44
**Source STATUS**: STATUS-2025-11-18-060044.md
**Source PLAN**: PLAN-2025-11-18-060344.md
**Sprint Duration**: 3 weeks (15 working days)
**Target Ship Date**: 2025-12-08

---

## Sprint Overview

**Current State**: Day 0.5 - Scaffolding complete, zero functional code
**Sprint Goal**: Ship v0.6.0 Template Discovery Engine with intelligent template recommendations
**Success Criteria**: All 30 tests passing, all 12 templates have metadata, performance < 500ms

---

## Week 1: Detection Infrastructure (Days 1-5)

### Day 1 (Monday): Docker Detection Foundation

**Time Budget**: 5 hours
**Goal**: Get first test passing (test_detect_docker_compose_project)

**Tasks**:
1. [ ] **Implement `_detect_docker()` method** (4 hours)
   - File: `src/mcpi/templates/discovery.py`
   - Parse docker-compose.yml with PyYAML
   - Extract service names
   - Set has_docker_compose, has_docker flags
   - Handle corrupted YAML gracefully
   - Code review: Check exception handling

2. [ ] **Update `detect()` orchestrator** (30 min)
   - Call `_detect_docker()` from detect()
   - Return ProjectContext with docker info

3. [ ] **Manual Verification** (30 min)
   - Create test project with docker-compose.yml
   - Run detector manually
   - Verify output matches expectations
   - Test with corrupted YAML

**Deliverables**:
- [ ] `_detect_docker()` implemented and working
- [ ] test_detect_docker_compose_project PASSING
- [ ] Manual verification successful

**Checkpoint**: End of day - 1 test passing, Docker detection works

---

### Day 2 (Tuesday): Language & Database Detection

**Time Budget**: 5 hours
**Goal**: Complete all detection methods

**Tasks**:
1. [ ] **Implement `_detect_language()` method** (3 hours)
   - File: `src/mcpi/templates/discovery.py`
   - Check for package.json (Node.js)
   - Check for requirements.txt, pyproject.toml (Python)
   - Check for go.mod (Go)
   - Check for Cargo.toml (Rust)
   - Handle multiple languages
   - Update detect() to call this method

2. [ ] **Implement `_detect_database()` method** (2 hours)
   - File: `src/mcpi/templates/discovery.py`
   - Extract databases from docker_services
   - Match postgres, mysql, mongodb, redis
   - Handle multiple databases
   - Update detect() to call this method

**Deliverables**:
- [ ] `_detect_language()` implemented
- [ ] `_detect_database()` implemented
- [ ] test_detect_nodejs_project PASSING
- [ ] test_detect_python_project PASSING
- [ ] test_detect_database_from_docker_services PASSING

**Checkpoint**: End of day - 4 tests passing, all detection methods work

---

### Day 3 (Wednesday): Complete Detection + Model Extension

**Time Budget**: 5 hours
**Goal**: All detection tests passing, ServerTemplate model extended

**Tasks**:
1. [ ] **Complete `detect()` orchestrator** (2 hours)
   - Ensure all detection methods called
   - Add path validation
   - Handle empty directories
   - Test with real projects
   - Run: pytest tests/test_template_discovery_functional.py::TestProjectDetection -v

2. [ ] **Extend ServerTemplate model** (1 hour)
   - File: `src/mcpi/templates/models.py`
   - Add best_for: list[str] field
   - Add keywords: list[str] field
   - Add recommendations: dict[str, Any] field
   - All optional (backward compatible)
   - Verify existing templates still load

3. [ ] **Start template metadata updates** (2 hours)
   - Update postgres/docker.yaml with metadata
   - Update postgres/local-development.yaml with metadata
   - Verify templates load correctly
   - Review metadata quality

**Deliverables**:
- [ ] All 7 detection tests PASSING
- [ ] ServerTemplate model extended
- [ ] 2/12 templates updated

**Checkpoint**: End of day - Detection complete, model extended

---

### Day 4 (Thursday): Template Metadata Updates

**Time Budget**: 5 hours
**Goal**: Complete template metadata for all 12 templates

**Tasks**:
1. [ ] **Update remaining postgres template** (30 min)
   - postgres/production.yaml

2. [ ] **Update github templates** (1.5 hours)
   - github/personal-full-access.yaml
   - github/read-only.yaml
   - github/public-repos.yaml

3. [ ] **Update filesystem templates** (1.5 hours)
   - filesystem/project-files.yaml
   - filesystem/user-documents.yaml
   - filesystem/custom-directories.yaml

4. [ ] **Update slack templates** (1 hour)
   - slack/bot-token.yaml
   - slack/limited-channels.yaml

5. [ ] **Update brave-search template** (30 min)
   - brave-search/api-key.yaml

**Deliverables**:
- [ ] 12/12 templates have metadata
- [ ] All templates load correctly
- [ ] Metadata validated by Pydantic

**Checkpoint**: End of day - All templates have metadata

---

### Day 5 (Friday): Detection Test Suite Completion

**Time Budget**: 5 hours
**Goal**: All detection tests passing, Week 1 complete

**Tasks**:
1. [ ] **Fix any failing detection tests** (3 hours)
   - Run full test suite
   - Debug failures
   - Fix edge cases
   - Ensure graceful error handling

2. [ ] **Manual verification with real projects** (2 hours)
   - Test 1: Node.js + Docker Compose project
   - Test 2: Python project without Docker
   - Test 3: Empty project
   - Document results

3. [ ] **Week 1 review** (30 min)
   - Review code quality
   - Check test coverage
   - Update documentation notes
   - Plan Week 2

**Deliverables**:
- [ ] 7/7 detection tests PASSING
- [ ] Manual verification successful (3 projects)
- [ ] Week 1 goals achieved

**Week 1 Checkpoint**: Detection infrastructure complete and working

---

## Week 2: Recommendation Engine (Days 6-10)

### Day 6 (Monday): Scoring Algorithm Implementation

**Time Budget**: 6 hours
**Goal**: Scoring algorithm working

**Tasks**:
1. [ ] **Implement `_score_template()` method** (4 hours)
   - File: `src/mcpi/templates/recommender.py`
   - Keyword matching logic (5 points per match)
   - Score boost from template recommendations
   - Return confidence score (0-100)
   - Generate reasoning list
   - Cap score at 100

2. [ ] **Manual tuning with test templates** (2 hours)
   - Test scoring with postgres templates
   - Adjust weights if needed
   - Verify sensible scores
   - Document scoring approach

**Deliverables**:
- [ ] `_score_template()` implemented
- [ ] Manual verification shows reasonable scores
- [ ] Scoring weights tuned

**Checkpoint**: End of day - Scoring algorithm works

---

### Day 7 (Tuesday): Scoring Tests + Algorithm Refinement

**Time Budget**: 5 hours
**Goal**: All scoring tests passing

**Tasks**:
1. [ ] **Refine scoring algorithm** (2 hours)
   - Fix any issues from manual testing
   - Improve reasoning messages
   - Add edge case handling

2. [ ] **Fix scoring unit tests** (3 hours)
   - test_score_docker_template_with_docker_project
   - test_score_local_template_without_docker
   - test_scoring_threshold
   - test_ranking_by_confidence
   - test_score_keyword_matching
   - Run: pytest tests/test_template_discovery_functional.py::TestTemplateScoring -v

**Deliverables**:
- [ ] 5/5 scoring tests PASSING
- [ ] Scoring algorithm refined
- [ ] Edge cases handled

**Checkpoint**: End of day - Scoring tests passing

---

### Day 8 (Wednesday): Recommendation API

**Time Budget**: 5 hours
**Goal**: `recommend()` method working end-to-end

**Tasks**:
1. [ ] **Implement `recommend()` method** (4 hours)
   - File: `src/mcpi/templates/recommender.py`
   - Use ProjectDetector to analyze project
   - Score all templates for server_id
   - Filter by min_confidence (default 10)
   - Return ranked list
   - Sort by confidence descending

2. [ ] **Manual verification** (1 hour)
   - Test with real project + postgres
   - Verify recommendations make sense
   - Check reasoning messages
   - Test with different project types

**Deliverables**:
- [ ] `recommend()` method implemented
- [ ] Manual verification successful
- [ ] Recommendations sensible

**Checkpoint**: End of day - Recommendation API works

---

### Day 9 (Thursday): Factory Functions + E2E Tests (Part 1)

**Time Budget**: 5 hours
**Goal**: Factory functions created, start E2E tests

**Tasks**:
1. [ ] **Create factory functions** (2 hours)
   - File: `src/mcpi/templates/recommender.py`
   - create_default_template_recommender()
   - create_test_template_recommender(template_manager)
   - Follow DIP pattern
   - Update tests to use factories

2. [ ] **Start E2E integration tests** (3 hours)
   - test_recommend_docker_postgres_for_docker_compose_project
   - test_recommend_local_postgres_for_non_docker_project
   - Fix any test failures
   - Run: pytest tests/test_template_discovery_functional.py::TestRecommendationEngine -v

**Deliverables**:
- [ ] Factory functions created
- [ ] Tests use factories
- [ ] 2/5 recommendation tests PASSING

**Checkpoint**: End of day - Factories working, E2E tests started

---

### Day 10 (Friday): E2E Tests Completion

**Time Budget**: 5 hours
**Goal**: All recommendation tests passing, Week 2 complete

**Tasks**:
1. [ ] **Complete E2E tests** (4 hours)
   - test_recommend_with_no_context
   - test_recommend_explains_reasoning
   - test_no_recommendations_scenario
   - Fix any failures
   - Ensure 100% passing

2. [ ] **Week 2 review** (1 hour)
   - Review code quality
   - Check test coverage on new code
   - Manual verification checkpoint
   - Verify recommendation quality acceptable
   - Plan Week 3

**Deliverables**:
- [ ] 5/5 recommendation tests PASSING
- [ ] Test coverage >= 95% on recommender code
- [ ] Week 2 goals achieved

**Week 2 Checkpoint**: Recommendation engine complete and working

---

## Week 3: CLI Integration & Ship (Days 11-15)

### Day 11 (Monday): --recommend Flag + Rich Display (Part 1)

**Time Budget**: 5 hours
**Goal**: --recommend flag added, start Rich display

**Tasks**:
1. [ ] **Add --recommend flag to CLI** (3 hours)
   - File: `src/mcpi/cli.py`
   - Add click option for --recommend
   - Add mutual exclusivity check with --template
   - Update help text
   - Add basic flow routing

2. [ ] **Start Rich display implementation** (2 hours)
   - Create _display_recommendations() function
   - Rich table with columns: Rank, Template, Confidence, Why
   - Color-code confidence levels
   - Test with sample recommendations

**Deliverables**:
- [ ] --recommend flag added
- [ ] Basic Rich display working
- [ ] Manual testing shows flag works

**Checkpoint**: End of day - Flag added, display started

---

### Day 12 (Tuesday): Rich Display + Wire Up Flow

**Time Budget**: 5 hours
**Goal**: Complete Rich display, wire up full recommendation flow

**Tasks**:
1. [ ] **Complete Rich display** (2 hours)
   - Add interactive prompt for selection
   - Handle "show all templates" option
   - Polish output formatting
   - Error handling for invalid selections

2. [ ] **Wire up recommendation flow** (3 hours)
   - Create _handle_recommendation_flow()
   - Integrate recommender with CLI
   - Handle user selection
   - Fall back to template list if needed
   - Error handling for no recommendations

**Deliverables**:
- [ ] Rich display complete and polished
- [ ] Full recommendation flow working
- [ ] Manual E2E testing successful

**Checkpoint**: End of day - CLI integration working end-to-end

---

### Day 13 (Wednesday): CLI Tests + Edge Cases + Documentation (Part 1)

**Time Budget**: 5 hours
**Goal**: CLI tests passing, edge cases handled, start documentation

**Tasks**:
1. [ ] **Fix CLI integration tests** (2 hours)
   - Run: pytest tests/test_template_recommendation_cli.py -v
   - Fix all failures
   - Ensure 15/15 CLI tests passing

2. [ ] **Edge case testing** (2 hours)
   - Empty project handling
   - Corrupted files handling
   - No recommendations scenario
   - Invalid server ID
   - All edge case tests passing

3. [ ] **Start documentation** (1 hour)
   - Update README.md with --recommend examples
   - Add basic usage guide

**Deliverables**:
- [ ] 15/15 CLI tests PASSING
- [ ] Edge case tests PASSING
- [ ] README.md partially updated

**Checkpoint**: End of day - All tests passing, docs started

---

### Day 14 (Thursday): Documentation + Performance + Pre-Ship Checklist

**Time Budget**: 5 hours
**Goal**: Documentation complete, performance verified, pre-ship checklist started

**Tasks**:
1. [ ] **Complete documentation** (1 hour)
   - Finish README.md updates
   - Update CLAUDE.md with Template Discovery section
   - Create template metadata authoring guide
   - Update CLI help text

2. [ ] **Performance testing** (2 hours)
   - Measure detection phase latency
   - Measure scoring phase latency
   - Measure total E2E latency
   - Verify < 500ms target met
   - Add performance tests
   - Document benchmarks

3. [ ] **Start pre-ship checklist** (2 hours)
   - Run full test suite (pytest -v)
   - Check test coverage (pytest --cov)
   - Verify no regressions
   - Review code quality
   - Update CHANGELOG

**Deliverables**:
- [ ] Documentation complete (4/4 files)
- [ ] Performance < 500ms verified
- [ ] Pre-ship checklist started

**Checkpoint**: End of day - Ready for final polish

---

### Day 15 (Friday): Final Polish + Manual E2E + Ship

**Time Budget**: 5 hours
**Goal**: Ship v0.6.0

**Tasks**:
1. [ ] **Final polish** (2 hours)
   - Code review all changes
   - Fix any minor issues
   - Clean up comments
   - Verify git history clean
   - Run: black src/ tests/ && ruff check src/ tests/ --fix

2. [ ] **Manual E2E verification** (2 hours)
   - Scenario 1: Docker Compose + postgres → postgres/docker
   - Scenario 2: Node.js without Docker → postgres/local
   - Scenario 3: Empty project → show all templates
   - Scenario 4: Invalid server → error message
   - Scenario 5: Corrupted YAML → graceful fallback
   - Scenario 6: Accept recommendation → proceeds
   - Scenario 7: Decline recommendation → show all
   - Scenario 8: --template overrides --recommend → error
   - Document results

3. [ ] **Ship v0.6.0** (1 hour)
   - Final test run (pytest)
   - Bump version to 0.6.0
   - Update CHANGELOG
   - Git commit: "feat: Add Template Discovery Engine v0.6.0"
   - Git tag: v0.6.0
   - Push to main
   - GitHub release

**Deliverables**:
- [ ] All 30 tests passing (100%)
- [ ] All 8 E2E scenarios verified
- [ ] v0.6.0 shipped and tagged

**Week 3 Checkpoint**: v0.6.0 SHIPPED ✅

---

## Sprint Metrics

### Daily Progress Tracking

| Day | Tasks Completed | Tests Passing | Blockers | Notes |
|-----|----------------|---------------|----------|-------|
| 1   | P0-1           | 1/30          | None     |       |
| 2   | P0-2, P0-3     | 4/30          | None     |       |
| 3   | P0-4, P0-5     | 7/30          | None     |       |
| 4   | P0-6           | 7/30          | None     |       |
| 5   | P2-1           | 7/30          | None     |       |
| 6   | P0-7           | 7/30          | None     |       |
| 7   | P2-2           | 12/30         | None     |       |
| 8   | P0-8           | 12/30         | None     |       |
| 9   | P0-9, P2-3 pt1 | 14/30         | None     |       |
| 10  | P2-3 pt2       | 17/30         | None     |       |
| 11  | P1-1, P1-2 pt1 | 17/30         | None     |       |
| 12  | P1-2 pt2, P1-3 | 17/30         | None     |       |
| 13  | P2-4, P2-5 pt1 | 30/30         | None     |       |
| 14  | P2-5 pt2, P2-6 | 30/30         | None     |       |
| 15  | P3-1, Ship     | 30/30         | None     |       |

### Weekly Goals

**Week 1**:
- Detection infrastructure complete
- 7/30 tests passing
- All 12 templates have metadata

**Week 2**:
- Recommendation engine complete
- 17/30 tests passing
- Factory functions created

**Week 3**:
- CLI integration complete
- 30/30 tests passing
- v0.6.0 shipped

---

## Daily Standup Template

### What I did yesterday:
- [ ] Task 1
- [ ] Task 2

### What I'm doing today:
- [ ] Task 1
- [ ] Task 2

### Blockers:
- None / [describe blocker]

### Tests passing:
- X/30 (Y% pass rate)

### Notes:
- [Any observations, concerns, or wins]

---

## Sprint Retrospective (Post-Ship)

### What went well:
- [To be filled after ship]

### What didn't go well:
- [To be filled after ship]

### What we learned:
- [To be filled after ship]

### Action items for next sprint:
- [To be filled after ship]

---

## Emergency Procedures

### If Blocked on Day 1-2 (Detection)

**Symptoms**: Docker detection not working, tests not passing

**Actions**:
1. Simplify detection (Docker Compose only, skip Dockerfile)
2. Add debug logging to see what's failing
3. Test with real docker-compose.yml files
4. Review PyYAML documentation
5. Ask for help if stuck > 2 hours

**Escalation**: If blocked for full day, reassess approach

---

### If Blocked on Day 6-8 (Scoring)

**Symptoms**: Scoring algorithm produces nonsensical results

**Actions**:
1. Simplify scoring (keyword matching only, skip boost)
2. Add debug output to see scores
3. Test with real templates and projects
4. Tune weights manually
5. Accept "good enough" for v0.6.0

**Escalation**: Ship simplified version if needed

---

### If Blocked on Day 11-12 (CLI)

**Symptoms**: CLI integration complex, tests failing

**Actions**:
1. Simplify UI (basic table, no interactive selection)
2. Focus on core workflow first
3. Defer polish to v0.6.1
4. Test manually before fixing tests

**Escalation**: Ship minimal CLI integration if needed

---

## Success Criteria Checklist

### Before Ship (Day 15)

#### Code Complete
- [ ] All 21 tasks completed
- [ ] All methods implemented (no NotImplementedError)
- [ ] All factory functions created
- [ ] All template metadata added (12/12)

#### Testing Complete
- [ ] All 30 discovery tests passing (100%)
- [ ] Test coverage >= 95% on new code
- [ ] No regressions (96.8%+ overall pass rate)
- [ ] All edge cases handled
- [ ] Performance < 500ms verified

#### Manual Verification Complete
- [ ] 8/8 E2E scenarios tested and working
- [ ] Real project testing successful
- [ ] User experience acceptable

#### Documentation Complete
- [ ] README.md updated
- [ ] CLAUDE.md updated
- [ ] Template authoring guide created
- [ ] CHANGELOG updated

#### Release Ready
- [ ] Version bumped to 0.6.0
- [ ] Git history clean
- [ ] All code formatted (black)
- [ ] All code linted (ruff)
- [ ] Ready to tag and push

---

**Last Updated**: 2025-11-18 06:03:44
**Sprint Status**: NOT STARTED (Day 0.5)
**Next Action**: Day 1 Task 1 - Implement `_detect_docker()` method
**Ship Target**: 2025-12-08 (End of Day 15)
