# MCPI Project Recovery Plan

**Source**: STATUS-2025-10-16-045045.md  
**Spec Version**: CLAUDE.md (MCPI MCP manager architecture)  
**Generated**: 2025-10-16 07:23:15  

## Executive Summary

The MCPI project suffers from a **critical test infrastructure breakdown** - while core functionality works manually, the testing system has completely collapsed due to architectural mismatch. The implementation is solid (35% real completion) but validation is broken (2.4% effective test rate). This plan prioritizes **fixing the broken test infrastructure first**, then systematically validating all core functionality.

**Key Issues**:
- Test architecture assumes `ClaudeCodeInstaller` but implementation uses `MCPManager`
- 827 lines of installer code (0% coverage) - completely untested
- 249 lines of registry management (0% coverage) - unvalidated
- CLI works manually but has 0% automated test coverage
- Planning documents describe wrong project ("claudew" vs "MCPI")

**Total Gap**: 65% remaining work, mostly validation and testing infrastructure

## P0 (Critical) - Test Infrastructure Recovery

### P0-1: Fix Broken CLI Test Architecture

**Status**: Not Started  
**Effort**: Medium (3-5 days)  
**Dependencies**: None  
**Spec Reference**: CLAUDE.md CLI Interface section • **Status Reference**: STATUS-2025-10-16-045045.md CLI Interface section

#### Description
All CLI tests fail because they mock `ClaudeCodeInstaller` but the actual CLI uses `MCPManager` from the plugin architecture. This is blocking all CLI validation and preventing accurate coverage measurement.

#### Acceptance Criteria
- [ ] Update all CLI test mocks to use `MCPManager` instead of `ClaudeCodeInstaller`
- [ ] Fix test imports to match actual CLI implementation (line 15: `from mcpi.clients import MCPManager`)
- [ ] Verify all CLI commands can be tested: list, search, info, install, uninstall, status, config
- [ ] Achieve >80% CLI test coverage with working integration
- [ ] All existing CLI tests pass (currently 0% pass rate)

#### Technical Notes
The CLI lazily initializes `MCPManager` via `get_mcp_manager()`. Tests need to mock the correct import path and respect the lazy loading pattern.

### P0-2: Establish Functional Installation Workflow Testing

**Status**: Not Started  
**Effort**: Large (1-2 weeks)  
**Dependencies**: P0-1  
**Spec Reference**: CLAUDE.md Installation Engine section • **Status Reference**: STATUS-2025-10-16-045045.md Installation Engine section

#### Description
All 827 lines of installer code have 0% test coverage. No verification that npm, python, git, or claude_code installation methods actually work end-to-end.

#### Acceptance Criteria
- [ ] Create working end-to-end tests for npm installer (95 lines, currently 0% coverage)
- [ ] Create working end-to-end tests for python installer (142 lines, currently 0% coverage)
- [ ] Create working end-to-end tests for git installer (154 lines, currently 0% coverage)
- [ ] Create working end-to-end tests for claude_code installer (155 lines, currently 0% coverage)
- [ ] Test complete workflows: discovery → selection → installation → configuration
- [ ] Verify rollback and error recovery for failed installations
- [ ] Achieve >90% coverage across all installer modules

#### Technical Notes
Focus on integration testing with proper mocking of external dependencies (git, npm, pip commands). Use temporary directories for safe testing.

### P0-3: Fix Registry Management Validation

**Status**: Not Started  
**Effort**: Medium (3-5 days)  
**Dependencies**: None  
**Spec Reference**: CLAUDE.md Registry System section • **Status Reference**: STATUS-2025-10-16-045045.md Server Registry System section

#### Description
Registry manager (249 lines, 0% coverage) and doc_parser (438 lines, 0% coverage) are completely untested despite being core functionality for server discovery and catalog management.

#### Acceptance Criteria
- [ ] Create comprehensive tests for RegistryManager class (registry/manager.py)
- [ ] Test registry loading, validation, and server lookup functionality
- [ ] Create tests for doc_parser module covering documentation extraction
- [ ] Test search and filtering functionality that works manually
- [ ] Verify registry.toml parsing and validation
- [ ] Achieve >85% coverage for both manager.py and doc_parser.py

#### Technical Notes
The registry system already works manually (shows 3 servers), focus on capturing existing behavior in tests.

## P1 (High) - Core Validation Completion

### P1-1: Configuration Management Testing

**Status**: Not Started  
**Effort**: Medium (3-5 days)  
**Dependencies**: P0-1  
**Spec Reference**: CLAUDE.md Configuration Management section • **Status Reference**: STATUS-2025-10-16-045045.md Configuration Management section

#### Description
Configuration system has minimal testing (11-19% coverage) despite being critical for profile management and cross-platform support.

#### Acceptance Criteria
- [ ] Increase manager.py coverage from 19% to >90%
- [ ] Increase profiles.py coverage from 11% to >90%
- [ ] Test server_state.py (153 lines, currently 0% coverage)
- [ ] Verify profile switching and template application
- [ ] Test cross-platform path resolution (Darwin, Linux, Windows)
- [ ] Validate Claude Code configuration updates

#### Technical Notes
Focus on the profile-based configuration system and template application mentioned in CLAUDE.md architecture.

### P1-2: Complete Integration Testing

**Status**: Not Started  
**Effort**: Large (1-2 weeks)  
**Dependencies**: P0-1, P0-2, P0-3  
**Spec Reference**: CLAUDE.md Testing Strategy section • **Status Reference**: STATUS-2025-10-16-045045.md integration test failures

#### Description
Create working end-to-end integration tests that validate complete workflows from discovery to installation to configuration.

#### Acceptance Criteria
- [ ] Create integration tests for discovery → installation → configuration workflow
- [ ] Test multi-server installation scenarios
- [ ] Verify configuration persistence and profile switching
- [ ] Test error scenarios and recovery mechanisms
- [ ] Create cross-platform integration tests
- [ ] Achieve end-to-end workflow validation for all major use cases

#### Technical Notes
Build on P0 foundation to create comprehensive integration scenarios that exercise the full system.

### P1-3: Client Plugin System Validation

**Status**: Not Started  
**Effort**: Medium (3-5 days)  
**Dependencies**: P0-1  
**Spec Reference**: CLAUDE.md MCP Client Plugin System • **Status Reference**: STATUS-2025-10-16-045045.md MCP Client Plugin System section

#### Description
Plugin system has inconsistent coverage (8-95% across modules) and broken CLI integration despite being the core architecture.

#### Acceptance Criteria
- [ ] Increase manager.py coverage from 12% to >90% (160 lines)
- [ ] Increase registry.py coverage from 12% to >90% (162 lines)
- [ ] Test MCPManager integration with CLI
- [ ] Verify ClientRegistry functionality
- [ ] Test ServerConfig and ServerState management
- [ ] Validate plugin discovery and loading

#### Technical Notes
The plugin architecture is the foundation of the system - ensure it's properly validated.

## P2 (Medium) - System Reliability

### P2-1: Cross-Platform Compatibility Testing

**Status**: Not Started  
**Effort**: Medium (3-5 days)  
**Dependencies**: P1-1  
**Spec Reference**: CLAUDE.md cross-platform support • **Status Reference**: STATUS-2025-10-16-045045.md cross-platform limitations

#### Description
Currently only validated on Darwin. Need testing for Windows and Linux path resolution and installation methods.

#### Acceptance Criteria
- [ ] Create Windows-specific tests for path resolution
- [ ] Create Linux-specific tests for path resolution
- [ ] Test installation methods on different platforms
- [ ] Verify configuration file locations per platform
- [ ] Test Claude Code integration across platforms

#### Technical Notes
Use platform detection and conditional testing to ensure compatibility.

### P2-2: Error Handling and Recovery

**Status**: Not Started  
**Effort**: Medium (3-5 days)  
**Dependencies**: P0-2, P1-2  
**Spec Reference**: CLAUDE.md error handling • **Status Reference**: STATUS-2025-10-16-045045.md Error Handling Completeness

#### Description
While CLI has good error handling patterns, installation and configuration error scenarios are untested.

#### Acceptance Criteria
- [ ] Test installation failure scenarios and rollback
- [ ] Test configuration corruption recovery
- [ ] Test network failure handling during registry updates
- [ ] Test permission failure scenarios
- [ ] Verify user-friendly error messages
- [ ] Test graceful degradation when dependencies missing

#### Technical Notes
Focus on realistic failure scenarios and recovery mechanisms.

## P3 (Low) - Enhancement and Optimization

### P3-1: Documentation Alignment

**Status**: Not Started  
**Effort**: Small (1-2 days)  
**Dependencies**: None  
**Spec Reference**: CLAUDE.md documentation • **Status Reference**: STATUS-2025-10-16-045045.md planning/implementation mismatch

#### Description
Archive obsolete planning documents that describe "claudew" instead of MCPI to prevent confusion.

#### Acceptance Criteria
- [ ] Archive outdated planning documents in .agent_planning/
- [ ] Update any references to "claudew" to "MCPI"
- [ ] Ensure all documentation aligns with implemented architecture
- [ ] Clean up technical debt files (cli_old.py, cli_optimized.py, etc.)

#### Technical Notes
Simple cleanup task to improve project clarity.

### P3-2: Performance Optimization Validation

**Status**: Not Started  
**Effort**: Small (1-2 days)  
**Dependencies**: P1-2  
**Spec Reference**: CLAUDE.md lazy loading patterns • **Status Reference**: STATUS-2025-10-16-045045.md performance optimizations

#### Description
Validate that lazy loading and performance optimizations actually improve startup time and user experience.

#### Acceptance Criteria
- [ ] Benchmark CLI startup time (currently claims 0.2s)
- [ ] Test lazy loading effectiveness
- [ ] Validate caching performance improvements
- [ ] Measure end-to-end operation performance
- [ ] Test performance under realistic loads

#### Technical Notes
Focus on measurable performance improvements.

## Dependency Graph

```
P0-1 (Fix CLI Tests) → P1-1 (Config Testing) → P2-1 (Cross-platform)
                     → P1-3 (Plugin System) → P2-2 (Error Handling)

P0-2 (Installation Testing) → P1-2 (Integration Testing) → P2-2 (Error Handling)

P0-3 (Registry Testing) → P1-2 (Integration Testing)

P3-1 (Documentation) [Independent]
P3-2 (Performance) → [Requires P1-2]
```

## Recommended Sprint Planning

### Sprint 1 (1 week): Critical Infrastructure Recovery
- P0-1: Fix CLI test architecture mismatch
- P0-3: Registry management validation
- Start P0-2: Installation workflow testing

### Sprint 2 (1 week): Installation Validation
- Complete P0-2: Installation workflow testing
- Start P1-1: Configuration management testing
- Start P1-3: Plugin system validation

### Sprint 3 (1 week): Integration Completion
- Complete P1-1 and P1-3
- P1-2: Complete integration testing
- P3-1: Documentation cleanup

### Sprint 4 (1 week): System Hardening
- P2-1: Cross-platform testing
- P2-2: Error handling validation
- P3-2: Performance validation

## Risk Assessment

**High Risk Items**:
- P0-2 may reveal fundamental installation architecture issues
- Integration testing may uncover additional architectural mismatches
- Cross-platform testing may require significant refactoring

**Medium Risk Items**:
- Configuration testing may reveal state management issues
- Plugin system validation may need architectural adjustments

**Low Risk Items**:
- Documentation alignment is straightforward
- Performance validation is measurement-focused

## Success Metrics

**Completion Criteria**:
- All CLI tests pass with >80% coverage
- All installer modules have >90% coverage with working end-to-end tests
- Registry operations fully validated (>85% coverage)
- Working integration tests for complete workflows
- Cross-platform compatibility verified

**Quality Gates**:
- No failing tests in core modules
- Coverage accurately reported (not inflated)
- Manual testing confirms automated test results
- Documentation accurately reflects implementation